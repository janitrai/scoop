{
  "payload_version": "v1",
  "source": "arxiv",
  "source_item_id": "arxiv.org_abs_2602.09001v1",
  "title": "DirMoE: Dirichlet-routed Mixture of Experts",
  "canonical_url": "http://arxiv.org/abs/2602.09001v1",
  "published_at": "2026-02-10T08:00:00Z",
  "source_domain": "arxiv.org",
  "source_metadata": {
    "collection": "ai_news",
    "job_name": "fetch_ai_news",
    "job_run_id": "backfill_2026-02-10_arxiv-072",
    "scraped_at": "2026-02-10T08:00:00Z",
    "scrape_run_uuid": "25eef9bb-0c19-5f71-9749-5d716a3c1538",
    "item_uuid": "64794883-92e1-57e5-a131-588c8e4947b6"
  },
  "body_text": "[cs.LG] Mixture-of-Experts (MoE) models have demonstrated exceptional performance in large-scale language models. Existing routers typically rely on non-differentiable Top-$k$+Softmax, limiting their"
}
