{
  "payload_version": "v1",
  "source": "arxiv",
  "source_item_id": "arxiv.org_abs_2602.10117v1",
  "title": "Biases in the Blind Spot: Detecting What LLMs Fail to Mention",
  "canonical_url": "http://arxiv.org/abs/2602.10117v1",
  "published_at": "2026-02-11T08:00:00Z",
  "source_domain": "arxiv.org",
  "source_metadata": {
    "collection": "ai_news",
    "job_name": "fetch_ai_news",
    "job_run_id": "backfill_2026-02-11_arxiv-061",
    "scraped_at": "2026-02-11T08:00:00Z",
    "scrape_run_uuid": "f9b8f6c2-84b0-52f8-b350-bca500039052",
    "item_uuid": "e36c78ba-f9f5-56b9-bee7-95d3698ae716"
  },
  "body_text": "[cs.LG, cs.AI] Large Language Models (LLMs) often provide chain-of-thought (CoT) reasoning traces that appear plausible, but may hide internal biases. We call these *unverbalized biases*. Monitoring m"
}
